import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
import concurrent.futures
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import numpy as np
import io

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“  ì„œë¹„ìŠ¤ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from services.dream_analyzer_service import DreamAnalyzerService
from services.report_generator_service import ReportGeneratorService
from services.image_generator_service import ImageGeneratorService
from services.stt_service import STTService

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ---
st.set_page_config(page_title="ë³´ì—¬DREAM", page_icon="ğŸŒ™", layout="wide")

@st.cache_resource
def initialize_services():
    """ API í‚¤ í™•ì¸, ëª¨ë“  ì„œë¹„ìŠ¤ ë° ëª¨ë¸ ê°ì²´ë“¤ì„ ìƒì„±í•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. """
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ)
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
    try:
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        retriever = vector_store.as_retriever()
        report_generator = ReportGeneratorService(api_key=api_key, retriever=retriever)
        dream_analyzer = DreamAnalyzerService(api_key=api_key)
        image_generator = ImageGeneratorService(api_key=api_key)
        stt_service = STTService(api_key=api_key)
        return report_generator, dream_analyzer, image_generator, stt_service
    except Exception as e:
        st.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.info("faiss_index í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

# --- 2. ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë…¹ìŒ ì²˜ë¦¬ í´ë˜ìŠ¤ (ì´ì „ê³¼ ë™ì¼) ---
class AudioFrameHandler(AudioProcessorBase):
    def __init__(self):
        self.audio_frames = []
    def recv(self, frame):
        self.audio_frames.append(frame.to_ndarray())
        return frame
    def get_audio_bytes(self):
        if not self.audio_frames: return None
        sound_chunk = np.concatenate(self.audio_frames)
        audio_data = (sound_chunk * 32767).astype(np.int16).tobytes()
        return io.BytesIO(audio_data)

# --- 3. ë¶„ì„ ë° ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ ê³µí†µ í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def run_analysis_pipeline(dream_text):
    """ ì…ë ¥ë°›ì€ í…ìŠ¤íŠ¸ë¡œ ì „ì²´ ë¶„ì„/ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ëª¨ë‘ í‘œì‹œí•©ë‹ˆë‹¤. """
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ì½”ë“œ)
    if not dream_text or "ì˜¤ë¥˜" in dream_text or "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in dream_text:
        st.error(dream_text or "ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    with st.spinner("RAGê°€ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì°¸ì¡°í•˜ì—¬ ê¿ˆì„ ì‹¬ì¸µ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        dream_report = st.session_state.report_generator.generate_report_with_rag(dream_text)
        nightmare_prompt = st.session_state.dream_analyzer.create_nightmare_prompt(dream_text)
        reconstructed_prompt, summary, mappings = st.session_state.dream_analyzer.create_reconstructed_prompt_and_analysis(dream_text, dream_report)
    with st.spinner("DALL-E 3ê°€ ê¿ˆì„ ì´ë¯¸ì§€ë¡œ ê·¸ë¦¬ê³  ìˆìŠµë‹ˆë‹¤... (1ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_nightmare = executor.submit(st.session_state.image_generator.generate_image_from_prompt, nightmare_prompt)
            future_reconstructed = executor.submit(st.session_state.image_generator.generate_image_from_prompt, reconstructed_prompt)
            nightmare_image_url = future_nightmare.result()
            reconstructed_image_url = future_reconstructed.result()
    st.subheader("ğŸ“ AI ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸")
    with st.container(border=True):
        st.markdown("##### ì‹¬ì¸µ ë¶„ì„ ìš”ì•½"); st.write(dream_report.get("analysis_summary", "ìš”ì•½ ì •ë³´ ì—†ìŒ"))
        st.markdown("##### ì£¼ìš” ê°ì •")
        for emo in dream_report.get("emotions", []): st.progress(emo['score'], text=f"{emo['emotion']} ({int(emo['score']*100)}%)")
        st.markdown("##### í•µì‹¬ í‚¤ì›Œë“œ"); st.write(" &nbsp; ".join(f"`{kw}`" for kw in dream_report.get("keywords", [])))
    st.divider()
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ì•…ëª½ì˜ ì‹œê°í™” (Before)")
        if nightmare_image_url.startswith("http"): st.image(nightmare_image_url, caption="AIê°€ ê·¸ë¦° ë‹¹ì‹ ì˜ ì•…ëª½")
        else: st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {nightmare_image_url}")
    with col2:
        st.subheader("ì¬êµ¬ì„±ëœ ê¿ˆ (After)")
        if reconstructed_image_url.startswith("http"): st.image(reconstructed_image_url, caption="AIê°€ ê¸ì •ì ìœ¼ë¡œ ì¬êµ¬ì„±í•œ ê¿ˆ")
        else: st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {reconstructed_image_url}")
    st.divider()
    st.subheader("âœ¨ ì´ë ‡ê²Œ ë°”ë€Œì—ˆì–´ìš”!"); st.write(summary)
    for mapping in mappings: st.markdown(f"- `{mapping['original']}` &nbsp; â¡ï¸ &nbsp; **`{mapping['transformed']}`**")

# --- 4. ë©”ì¸ ì•± ì‹¤í–‰ ---
def main():
    st.title("ë³´ì—¬DREAM ğŸŒ™")
    st.write("ë‹¹ì‹ ì˜ ê¿ˆ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”. AIê°€ ì•…ëª½ì„ ë¶„ì„í•˜ê³  ê¸ì •ì ì¸ ì´ë¯¸ì§€ë¡œ ì¬êµ¬ì„±í•´ ë“œë¦½ë‹ˆë‹¤.")
    
    # ì„œë¹„ìŠ¤ ê°ì²´ ì´ˆê¸°í™”
    st.session_state.report_generator, st.session_state.dream_analyzer, st.session_state.image_generator, st.session_state.stt_service = initialize_services()

    # --- ğŸ”½ ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ ì‚¬í•­ ğŸ”½ ---
    # ì„¸ì…˜ ìƒíƒœì— 'transcribed_text'ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”
    if "transcribed_text" not in st.session_state:
        st.session_state.transcribed_text = ""
    # --- ğŸ”¼ ì—¬ê¸°ê°€ í•µì‹¬ ë³€ê²½ ì‚¬í•­ ğŸ”¼ ---

    tab1, tab2, tab3 = st.tabs(["âœï¸ í…ìŠ¤íŠ¸ë¡œ ì…ë ¥", "â¬†ï¸ íŒŒì¼ ì—…ë¡œë“œ", "ğŸ¤ ì‹¤ì‹œê°„ ë…¹ìŒ"])

    with tab1:
        dream_text_input = st.text_area("ì–´ì ¯ë°¤ ì–´ë–¤ ê¿ˆì„ ê¾¸ì…¨ë‚˜ìš”?", height=200, placeholder="ì—¬ê¸°ì— ê¿ˆ ë‚´ìš©ì„ ìì„¸íˆ ì ì–´ì£¼ì„¸ìš”...", key="text_input")
        if st.button("ë¶„ì„ ì‹œì‘ (í…ìŠ¤íŠ¸)", type="primary", use_container_width=True):
            run_analysis_pipeline(dream_text_input)

    with tab2:
        uploaded_file = st.file_uploader("ìŒì„± íŒŒì¼(mp3, wav, m4a ë“±)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=['mp3', 'm4a', 'wav', 'ogg'], key="file_uploader")
        if uploaded_file is not None:
            if st.button("ì´ íŒŒì¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜í•˜ê¸°", use_container_width=True):
                with st.spinner("ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì¤‘ì…ë‹ˆë‹¤..."):
                    audio_bytes = uploaded_file.getvalue()
                    st.session_state.transcribed_text = st.session_state.stt_service.transcribe_from_bytes(audio_bytes)
        
        # ì„¸ì…˜ì— ì €ì¥ëœ í…ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ ì˜ì—­ì— í‘œì‹œ
        st.text_area("**ë³€í™˜ëœ í…ìŠ¤íŠ¸**", value=st.session_state.transcribed_text, height=150, key="transcribed_text_file")
        if st.session_state.transcribed_text:
             if st.button("ë¶„ì„ ì‹œì‘ (ì—…ë¡œë“œ íŒŒì¼)", type="primary", use_container_width=True):
                run_analysis_pipeline(st.session_state.transcribed_text)
                
    with tab3:
        st.write("ì•„ë˜ 'START' ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë§ˆì´í¬ì— ê¿ˆ ì´ì•¼ê¸°ë¥¼ ë…¹ìŒí•˜ì„¸ìš”. ë…¹ìŒì„ ë©ˆì¶”ë ¤ë©´ 'STOP'ì„ ëˆ„ë¥´ì„¸ìš”.")
        webrtc_ctx = webrtc_streamer(key="audio-recorder", mode=WebRtcMode.SENDONLY, audio_processor_factory=AudioFrameHandler)
        
        if webrtc_ctx.audio_processor:
            if st.button("ë…¹ìŒ ë‚´ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜", use_container_width=True):
                audio_bytes_io = webrtc_ctx.audio_processor.get_audio_bytes()
                if audio_bytes_io:
                    with st.spinner("ë…¹ìŒëœ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤..."):
                        st.session_state.transcribed_text = st.session_state.stt_service.transcribe_from_bytes(audio_bytes_io.getvalue())
                else:
                    st.warning("ë…¹ìŒëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. START ë²„íŠ¼ì„ ëˆ„ë¥´ê³  ë…¹ìŒì„ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”.")
        
        # ì„¸ì…˜ì— ì €ì¥ëœ í…ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ ì˜ì—­ì— í‘œì‹œ
        st.text_area("**ë³€í™˜ëœ í…ìŠ¤íŠ¸**", value=st.session_state.transcribed_text, height=150, key="transcribed_text_mic")
        if st.session_state.transcribed_text:
            if st.button("ë¶„ì„ ì‹œì‘ (ë…¹ìŒ ë‚´ìš©)", type="primary", use_container_width=True):
                run_analysis_pipeline(st.session_state.transcribed_text)

if __name__ == "__main__":
    main()